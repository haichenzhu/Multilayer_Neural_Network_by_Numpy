{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Me\n",
    "\n",
    "Please **execute the code blocks below one by one**, and the performance of training and the prediction of test data set will be generate at the last code block.\n",
    "\n",
    "The markdown note before each code block describe the content of each code block\n",
    "\n",
    "The predicted results can be opened by the following code:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "with h5py.File('../Output/Predicted_labels.h5','r') as H:\n",
    "\n",
    "    test_label = np.copy(H['label'])\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the activation class, including activations like: linear, tanh, logistic, relu, sigmoid, softmax and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def __linear(self, x):\n",
    "        return x\n",
    "    \n",
    "    def __linear_deriv(self, a):\n",
    "        return 1\n",
    "    \n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        '''partial derivative for each element of data under tanh(data)'''\n",
    "        return 1.0 - a ** 2\n",
    "           \n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        return  a * (1 - a)\n",
    "    \n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "        \n",
    "    def __relu_deriv(self, a):\n",
    "        a[a <= 0] = 0\n",
    "        a[a > 0] = 1\n",
    "        return a\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1.0/(1.0 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_deriv(self, a):\n",
    "        return a * (1.0 - a)\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        '''\n",
    "        activation of output layer\n",
    "        In order to avoid the parameters being too large, \n",
    "        subtract the largest one, which is equivalent to each one being negative.\n",
    "        '''\n",
    "\n",
    "        x = np.atleast_2d(x)\n",
    "        exp = np.exp(np.subtract(x.T, x.max(axis=1))).T\n",
    "        return np.divide(exp, np.sum(exp, axis=1).reshape(-1, 1))\n",
    "        \n",
    "    def __softmax_deriv(self, a):\n",
    "        '''\n",
    "        partial derivative for each element of data under softmax(data)\n",
    "        '''\n",
    "        return a - a ** 2\n",
    "    \n",
    "    def __init__(self, activation='relu'):\n",
    "        if activation == 'linear':\n",
    "            self.f = self.__linear\n",
    "            self.f_deriv = self.__linear_deriv\n",
    "        elif activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'sigmoid':\n",
    "            self.f = self.__sigmoid\n",
    "            self.f_deriv = self.__sigmoid_deriv\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HiddenLayer class, with every instance record its parameters (W, b), gradients (grad_W, grad_b), last gradients (W_v, b_v), activation and the derivative of activation of last layer. They have two functions: forward and backward. It will perform different forward based on the training_process flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer=None,activation=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: \n",
    "        Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "        Hidden unit activation is given by: activation(dot(input, W) + b)\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input = None\n",
    "        self.activation = Activation(activation).f\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv = None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv = Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        # initialize W with Xavier_uniform\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "        self.b = np.zeros(n_out, )\n",
    "        \n",
    "        # initialize space for gradient descent\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        # record last gradient for momemtum\n",
    "        self.W_v = np.zeros(self.W.shape)\n",
    "        self.b_v = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, train_process, dropout):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        :type train_process: boolean\n",
    "        :param train_process: a flag tell the layer it is now training or not\n",
    "        :type dropout: float\n",
    "        :param dropout: the probability of units shutdown\n",
    "        '''\n",
    "        # if it is training process, every unit has probability of dropout shut down\n",
    "        if train_process:\n",
    "            # random select shutdown units according to dropout\n",
    "            selection = np.random.binomial(1, 1 - dropout, input.shape[-1])\n",
    "            input = selection * input\n",
    "            lin_output = np.dot(input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output) \n",
    "            )\n",
    "            self.input = input\n",
    "        \n",
    "        # if it is not training, scale down each parameter by (1 - dropout)\n",
    "        else:\n",
    "            \n",
    "            lin_output = np.dot(input, (1 - dropout) * self.W) + (1 - dropout) * self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "            self.input = input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        '''\n",
    "        backward to update each layer's gradients according to delta\n",
    "        '''\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.mean(np.atleast_2d(delta), axis=0)\n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MultiLayerPerceptron class, which initial with instance of HiddenLayer. It has the following function: prepare-batch_index, forward (forward propagation), criterion_CE (to calculate crossentropy loss and delta of last activation), backward and update (for backward propagation), fit(train the model), and finaly, predict(get the result of prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "     \n",
    "    def __init__(self, layers, activation=[None, 'relu', 'softmax']):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"relu\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        # initialize layers\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        \n",
    "        # initialize activation and activation_deriv of last layer to each layer\n",
    "        self.activation = activation\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(HiddenLayer(layers[i], layers[i + 1], activation[i], activation[i + 1]))\n",
    "    \n",
    "    \n",
    "    def __prepare_batch_index(self, length, batch_size):\n",
    "        '''\n",
    "        random select training batch data\n",
    "        return index list of every batch\n",
    "        :type length: int\n",
    "        :param length: number of training data\n",
    "        :type batch_size: int\n",
    "        :param batch_size: number of batch training data\n",
    "        ''' \n",
    "        # initialize a list, with every element recording the index of one batch\n",
    "        batch_index_list = []\n",
    "        # total index wait to be selected\n",
    "        full_index = np.arange(length)\n",
    "\n",
    "        for _ in range(length // batch_size):\n",
    "            # random select index for current batch\n",
    "            batch_index = np.random.choice(full_index, batch_size, replace=False)\n",
    "            batch_index_list.append(batch_index.tolist())\n",
    "            \n",
    "            # find the index of selected\n",
    "            i_list = np.array([], dtype=int)\n",
    "            for i in batch_index:\n",
    "                i_list = np.concatenate((i_list, np.where(full_index == i)[0]), axis=None)\n",
    "            \n",
    "            # remove these index from total index array\n",
    "            full_index = np.delete(full_index, i_list)\n",
    "        \n",
    "        # append the rest index, so that for every epoch, every training data is trained\n",
    "        if full_index.size > 0:\n",
    "            batch_index_list.append(full_index.tolist())\n",
    "\n",
    "        return batch_index_list\n",
    "    \n",
    "      \n",
    "    def forward(self, input, dropout=0, train_process=False):\n",
    "        '''\n",
    "        with given input, forward propagation to calculate result\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        :type train_process: boolean\n",
    "        :param train_process: a flag tell the layer it is now training or not, default False\n",
    "        :type dropout: float\n",
    "        :param dropout: the probability of units shutdown, default 0\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(input, train_process, dropout)\n",
    "            input = output\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def criterion_CE(self, y, y_hat):\n",
    "        '''\n",
    "        calculate crossentropy loss and the delta of last layer\n",
    "        :type y: np.array\n",
    "        :param y: true value of class under onehot encoder\n",
    "        :type y_hat: np.array\n",
    "        :param y_hat: the predicted result (the output of last layer)\n",
    "        '''\n",
    "        # convert to 2d numpy array\n",
    "        y = np.array(np.atleast_2d(y), dtype=float)\n",
    "        y_hat = np.array(np.atleast_2d(y_hat), dtype=float)\n",
    "        \n",
    "        # in case the error of log(0), replace them with very small value\n",
    "        y_hat[y_hat == 0] = 1e-10\n",
    "        \n",
    "        # calculate crossentropy loss\n",
    "        loss = -np.sum(y * np.log(y_hat), axis=1)\n",
    "        \n",
    "        # the delta of last activation to last layer\n",
    "        delta = y_hat - y\n",
    "\n",
    "        return np.mean(loss), delta\n",
    "        \n",
    "    \n",
    "    def backward(self, delta):\n",
    "        '''\n",
    "        backward propagation with delta layer by layer\n",
    "        :type delta: np.array\n",
    "        :param delta: the delta of last layer\n",
    "        '''\n",
    "        delta = self.layers[-1].backward(delta, output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta = layer.backward(delta)\n",
    "            \n",
    "    \n",
    "    def update(self, lr, weight_decay, momentum):\n",
    "        '''\n",
    "        update each layer's parameter\n",
    "        :type weight_decay: float\n",
    "        :param weight_decay: force the model parameter to be small\n",
    "        :type momentum: float\n",
    "        :param momentum: how much it will be affected by last gradient\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            layer.W_v = momentum * layer.W_v + lr * layer.grad_W\n",
    "            layer.b_v = momentum * layer.b_v + lr * layer.grad_b\n",
    "            \n",
    "            layer.W = (1 - lr * weight_decay) * layer.W - layer.W_v\n",
    "            layer.b = (1 - lr * weight_decay) * layer.b - layer.b_v\n",
    "            \n",
    "    \n",
    "    def fit(self, X_train, y_train, X_validation, y_validation, learning_rate=0.1, epochs=100, weight_decay=0, momentum=0, dropout=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X_train: (np.array) Input data or features\n",
    "        :param y_train: (np.array) Input targets\n",
    "        :param X_validation: (np.array) Validation data or features\n",
    "        :param y_validation: (np.array) Validation targets\n",
    "        :param learning_rate: (float) parameters defining the speed of learning\n",
    "        :param epochs: (int) number of times the dataset is presented to the network for learning\n",
    "        :param weight_decay: (float) adding l2 penalty, force the model parameters to be small\n",
    "        :param momentum: (float) how much it will be affected by last gradient\n",
    "        :param dropout: (float) the probability of units shutdown\n",
    "        :param batch_size: 0 or None or nb_of_training-----------Batch Gradient Descent\n",
    "                           1-------------------------------------Stochastic Gradient Descent\n",
    "                           1 < batch_size < nb_of_training-------Mini-batch Gradient Descent\n",
    "        \"\"\" \n",
    "        # checking the validation of hyperparameters\n",
    "        if weight_decay < 0 or weight_decay > 1:\n",
    "            print('Invalid weight_decay value, it should be in [0, 1]')\n",
    "            return\n",
    "        if momentum < 0 or momentum > 1:\n",
    "            print('Invalid momentum value, it should be in [0, 1]')\n",
    "            return\n",
    "        if dropout < 0 or dropout > 1:\n",
    "            print('Invalid dropout value, it should be in [0, 1]')\n",
    "            return\n",
    "        if batch_size < 0 or batch_size > X_train.shape[0]:\n",
    "            print('Invalid batch_size value, it should be in [0, X_train.shape[0]]')\n",
    "            return\n",
    "\n",
    "        # initial space to record training performance\n",
    "        train_loss_return = np.zeros(epochs)\n",
    "        train_acc_return = np.zeros(epochs)\n",
    "        validation_loss_return = np.zeros(epochs)\n",
    "        validation_acc_return = np.zeros(epochs)\n",
    "        epoch_num = []\n",
    "\n",
    "        # if using batch gradient descent\n",
    "        if not batch_size or batch_size == X_train.shape[0]:\n",
    "            for k in range(epochs):\n",
    "                epoch_num.append(k + 1)\n",
    "\n",
    "                # forward propagation\n",
    "                y_hat = self.forward(X_train, dropout=dropout, train_process=True)\n",
    "\n",
    "                # backward propagation\n",
    "                batch_loss, delta = self.criterion_CE(onehot[y_train], y_hat)\n",
    "                self.backward(delta)\n",
    "\n",
    "                # update             \n",
    "                self.update(learning_rate, weight_decay, momentum)\n",
    "                \n",
    "                # record training loss\n",
    "                train_loss_return[k] = batch_loss\n",
    "\n",
    "                # calculate training accuracy\n",
    "                train_prediction = self.predict(X_train)\n",
    "                train_acc_return[k] = np.sum(train_prediction == y_train) / y_train.shape[0]\n",
    "\n",
    "                # forward for validation set for calculating loss and accuracy\n",
    "                output = self.forward(X_validation)\n",
    "                validation_loss_return[k], _ = self.criterion_CE(onehot[y_validation], output)\n",
    "                validation_acc_return[k] = np.sum(np.argmax(output, axis=1) == y_validation) / y_validation.shape[0]\n",
    "                \n",
    "                # display training performance\n",
    "                print('Epoch:{0: 4d}, Training Loss:{1: .4f}, Training Accuracy:{2: .4f}, Validation Loss:{3: .4f}, Validation Accuracy:{4: .4f}'\\\n",
    "                    .format(k + 1, train_loss_return[k], train_acc_return[k], validation_loss_return[k], validation_acc_return[k]))\n",
    "            print('Finished Training!')\n",
    "        \n",
    "        # if specify batch_siaze\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                epoch_num.append(k + 1)\n",
    "                loss = []\n",
    "                \n",
    "                # prepare batch index\n",
    "                batch_index_list = self.__prepare_batch_index(X_train.shape[0], batch_size)\n",
    "                for index_list in batch_index_list:\n",
    "                    X_train_batch = X_train[index_list, :]\n",
    "                    y_train_batch = y_train[index_list]\n",
    "                    \n",
    "                    # forward propagation\n",
    "                    y_hat = self.forward(X_train_batch, dropout=dropout, train_process=True)    \n",
    "                    \n",
    "                    # backward propagation\n",
    "                    batch_loss, delta = self.criterion_CE(onehot[y_train_batch], y_hat)\n",
    "                    loss.append(batch_loss)  \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate, weight_decay, momentum)\n",
    "                \n",
    "                # record loss\n",
    "                train_loss_return[k] = np.mean(loss)\n",
    "\n",
    "                # calculate training accuracy\n",
    "                train_prediction = self.predict(X_train)\n",
    "                train_acc_return[k] = np.sum(train_prediction == y_train) / y_train.shape[0]\n",
    "                \n",
    "                # forward for validation set for calculating loss and accuracy\n",
    "                output = self.forward(X_validation)\n",
    "                validation_loss_return[k], _ = self.criterion_CE(onehot[y_validation], output)\n",
    "                validation_acc_return[k] = np.sum(np.argmax(output, axis=1) == y_validation) / y_validation.shape[0]\n",
    "                \n",
    "                # display training performance\n",
    "                print('Epoch:{0: 4d}, Training Loss:{1: .4f}, Training Accuracy:{2: .4f}, Validation Loss:{3: .4f}, Validation Accuracy:{4: .4f}'\\\n",
    "                    .format(k + 1, train_loss_return[k], train_acc_return[k], validation_loss_return[k], validation_acc_return[k]))\n",
    "            print('Finished Training!')\n",
    "\n",
    "        return epoch_num, train_loss_return, train_acc_return, validation_loss_return, validation_acc_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        predict the label\n",
    "        '''\n",
    "        output = np.argmax(self.forward(x), axis=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from ../Input/ directory, prepare training and validation data set, preprocessing with whitening, get the onehot encoder based on the number of class to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from required directory\n",
    "with h5py.File('../Input/train_128.h5','r') as H:\n",
    "    train_data = np.copy(H['data'])\n",
    "with h5py.File('../Input/train_label.h5','r') as H:\n",
    "    train_label = np.copy(H['label'])\n",
    "with h5py.File('../Input/test_128.h5','r') as H:\n",
    "    test_data = np.copy(H['data']) \n",
    "\n",
    "    \n",
    "def plot_result(epoch_num, train_loss, train_acc, validation_loss, validation_acc):\n",
    "    '''\n",
    "    plot the performance of training process\n",
    "    '''\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and validation Loss')\n",
    "    plt.plot(epoch_num, train_loss, color='red', label='train')\n",
    "    plt.plot(epoch_num, validation_loss, color='blue', label='validation')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and validation Accuracy')\n",
    "    plt.plot(epoch_num, train_acc, color='red', label='train')\n",
    "    plt.plot(epoch_num, validation_acc, color='blue', label='validation')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "\n",
    "def train_validation_split(X, y, validation_ratio):\n",
    "    '''\n",
    "    randomly split the data set into training and validation data set\n",
    "    :param validation_ratio: (float) how much\n",
    "    '''\n",
    "    # check the validation of validation_ratio\n",
    "    if validation_ratio <= 0 or validation_ratio >= 1:\n",
    "        print('Invalid validation_ratio, it should be in (1, 0).')\n",
    "        return\n",
    "    \n",
    "    # get shuffled index\n",
    "    shuffle_indexes = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    # number of validation data\n",
    "    validation_size = int(X.shape[0] * validation_ratio)\n",
    "    \n",
    "    # the index of training and validation data\n",
    "    validation_indexes = shuffle_indexes[:validation_size]\n",
    "    train_indexes = shuffle_indexes[validation_size:]\n",
    "    \n",
    "    # get them\n",
    "    X_train = X[train_indexes]\n",
    "    y_train = y[train_indexes]\n",
    "    X_validation = X[validation_indexes]\n",
    "    y_validation = y[validation_indexes]\n",
    "    \n",
    "    return X_train, y_train, X_validation, y_validation\n",
    "\n",
    "\n",
    "# prepare training and validation data set with ratio 8 : 2\n",
    "X_train, y_train, X_validation, y_validation = train_validation_split(train_data, train_label, 0.2)\n",
    "X_test = test_data\n",
    "\n",
    "# calculate the mean and standard deviation for whitening\n",
    "X_train_mean = np.mean(X_train, axis=0)\n",
    "X_train_std = np.std(X_train, axis=0)\n",
    "\n",
    "# data preprocessing with whitening\n",
    "X_train_whitening = (X_train - X_train_mean) / X_train_std\n",
    "X_validation_whitening = (X_validation - X_train_mean) / X_train_std\n",
    "X_test_whitening = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "# onehot encoder\n",
    "onehot = np.eye(len(set(train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the multilayer perceptron, and start training process, then print the time cost and plot the performance, finally, get the prediction of test data set, and save it into ../Output/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1, Training Loss: 1.9389, Training Accuracy: 0.6471, Validation Loss: 0.9785, Validation Accuracy: 0.6416\n",
      "Epoch:   2, Training Loss: 1.0802, Training Accuracy: 0.8023, Validation Loss: 0.5757, Validation Accuracy: 0.8023\n",
      "Epoch:   3, Training Loss: 0.7924, Training Accuracy: 0.8253, Validation Loss: 0.5214, Validation Accuracy: 0.8219\n",
      "Epoch:   4, Training Loss: 0.6941, Training Accuracy: 0.8290, Validation Loss: 0.5207, Validation Accuracy: 0.8240\n",
      "Epoch:   5, Training Loss: 0.6519, Training Accuracy: 0.8487, Validation Loss: 0.4534, Validation Accuracy: 0.8437\n",
      "Epoch:   6, Training Loss: 0.6306, Training Accuracy: 0.8553, Validation Loss: 0.4465, Validation Accuracy: 0.8528\n",
      "Epoch:   7, Training Loss: 0.5797, Training Accuracy: 0.8608, Validation Loss: 0.4299, Validation Accuracy: 0.8556\n",
      "Epoch:   8, Training Loss: 0.5625, Training Accuracy: 0.8648, Validation Loss: 0.4244, Validation Accuracy: 0.8599\n",
      "Epoch:   9, Training Loss: 0.5522, Training Accuracy: 0.8680, Validation Loss: 0.4178, Validation Accuracy: 0.8592\n",
      "Epoch:  10, Training Loss: 0.5328, Training Accuracy: 0.8710, Validation Loss: 0.4178, Validation Accuracy: 0.8626\n",
      "Epoch:  11, Training Loss: 0.5326, Training Accuracy: 0.8704, Validation Loss: 0.4150, Validation Accuracy: 0.8623\n",
      "Epoch:  12, Training Loss: 0.5165, Training Accuracy: 0.8724, Validation Loss: 0.4080, Validation Accuracy: 0.8656\n",
      "Epoch:  13, Training Loss: 0.5052, Training Accuracy: 0.8770, Validation Loss: 0.3934, Validation Accuracy: 0.8698\n",
      "Epoch:  14, Training Loss: 0.4952, Training Accuracy: 0.8770, Validation Loss: 0.3952, Validation Accuracy: 0.8669\n",
      "Epoch:  15, Training Loss: 0.4794, Training Accuracy: 0.8809, Validation Loss: 0.3863, Validation Accuracy: 0.8729\n",
      "Epoch:  16, Training Loss: 0.4833, Training Accuracy: 0.8817, Validation Loss: 0.3838, Validation Accuracy: 0.8720\n",
      "Epoch:  17, Training Loss: 0.4696, Training Accuracy: 0.8823, Validation Loss: 0.3890, Validation Accuracy: 0.8709\n",
      "Epoch:  18, Training Loss: 0.4636, Training Accuracy: 0.8841, Validation Loss: 0.3779, Validation Accuracy: 0.8765\n",
      "Epoch:  19, Training Loss: 0.4607, Training Accuracy: 0.8863, Validation Loss: 0.3745, Validation Accuracy: 0.8770\n",
      "Epoch:  20, Training Loss: 0.4613, Training Accuracy: 0.8870, Validation Loss: 0.3724, Validation Accuracy: 0.8762\n",
      "Epoch:  21, Training Loss: 0.4636, Training Accuracy: 0.8891, Validation Loss: 0.3717, Validation Accuracy: 0.8792\n",
      "Epoch:  22, Training Loss: 0.4567, Training Accuracy: 0.8885, Validation Loss: 0.3785, Validation Accuracy: 0.8767\n",
      "Epoch:  23, Training Loss: 0.4448, Training Accuracy: 0.8891, Validation Loss: 0.3728, Validation Accuracy: 0.8796\n",
      "Epoch:  24, Training Loss: 0.4358, Training Accuracy: 0.8885, Validation Loss: 0.3766, Validation Accuracy: 0.8776\n",
      "Epoch:  25, Training Loss: 0.4439, Training Accuracy: 0.8903, Validation Loss: 0.3797, Validation Accuracy: 0.8782\n",
      "Epoch:  26, Training Loss: 0.4433, Training Accuracy: 0.8874, Validation Loss: 0.3873, Validation Accuracy: 0.8757\n",
      "Epoch:  27, Training Loss: 0.4480, Training Accuracy: 0.8914, Validation Loss: 0.3725, Validation Accuracy: 0.8803\n",
      "Epoch:  28, Training Loss: 0.4248, Training Accuracy: 0.8942, Validation Loss: 0.3645, Validation Accuracy: 0.8798\n",
      "Epoch:  29, Training Loss: 0.4343, Training Accuracy: 0.8954, Validation Loss: 0.3605, Validation Accuracy: 0.8802\n",
      "Epoch:  30, Training Loss: 0.4279, Training Accuracy: 0.8955, Validation Loss: 0.3676, Validation Accuracy: 0.8792\n",
      "Epoch:  31, Training Loss: 0.4234, Training Accuracy: 0.8964, Validation Loss: 0.3634, Validation Accuracy: 0.8812\n",
      "Epoch:  32, Training Loss: 0.4270, Training Accuracy: 0.8971, Validation Loss: 0.3605, Validation Accuracy: 0.8848\n",
      "Epoch:  33, Training Loss: 0.4275, Training Accuracy: 0.8968, Validation Loss: 0.3669, Validation Accuracy: 0.8823\n",
      "Epoch:  34, Training Loss: 0.4165, Training Accuracy: 0.8980, Validation Loss: 0.3582, Validation Accuracy: 0.8804\n",
      "Epoch:  35, Training Loss: 0.4183, Training Accuracy: 0.8963, Validation Loss: 0.3635, Validation Accuracy: 0.8791\n",
      "Epoch:  36, Training Loss: 0.4277, Training Accuracy: 0.8973, Validation Loss: 0.3642, Validation Accuracy: 0.8815\n",
      "Epoch:  37, Training Loss: 0.4203, Training Accuracy: 0.8998, Validation Loss: 0.3622, Validation Accuracy: 0.8819\n",
      "Epoch:  38, Training Loss: 0.4220, Training Accuracy: 0.8997, Validation Loss: 0.3593, Validation Accuracy: 0.8844\n",
      "Epoch:  39, Training Loss: 0.4183, Training Accuracy: 0.9002, Validation Loss: 0.3562, Validation Accuracy: 0.8831\n",
      "Epoch:  40, Training Loss: 0.4193, Training Accuracy: 0.9001, Validation Loss: 0.3541, Validation Accuracy: 0.8858\n",
      "Epoch:  41, Training Loss: 0.4131, Training Accuracy: 0.8995, Validation Loss: 0.3567, Validation Accuracy: 0.8834\n",
      "Epoch:  42, Training Loss: 0.4128, Training Accuracy: 0.8988, Validation Loss: 0.3710, Validation Accuracy: 0.8818\n",
      "Epoch:  43, Training Loss: 0.4038, Training Accuracy: 0.8996, Validation Loss: 0.3684, Validation Accuracy: 0.8815\n",
      "Epoch:  44, Training Loss: 0.4145, Training Accuracy: 0.8987, Validation Loss: 0.3647, Validation Accuracy: 0.8790\n",
      "Epoch:  45, Training Loss: 0.4057, Training Accuracy: 0.9009, Validation Loss: 0.3536, Validation Accuracy: 0.8838\n",
      "Epoch:  46, Training Loss: 0.3998, Training Accuracy: 0.9015, Validation Loss: 0.3552, Validation Accuracy: 0.8814\n",
      "Epoch:  47, Training Loss: 0.4103, Training Accuracy: 0.9003, Validation Loss: 0.3636, Validation Accuracy: 0.8822\n",
      "Epoch:  48, Training Loss: 0.4059, Training Accuracy: 0.9015, Validation Loss: 0.3486, Validation Accuracy: 0.8824\n",
      "Epoch:  49, Training Loss: 0.3985, Training Accuracy: 0.9031, Validation Loss: 0.3500, Validation Accuracy: 0.8833\n",
      "Epoch:  50, Training Loss: 0.4054, Training Accuracy: 0.9006, Validation Loss: 0.3603, Validation Accuracy: 0.8799\n",
      "Epoch:  51, Training Loss: 0.4037, Training Accuracy: 0.9043, Validation Loss: 0.3503, Validation Accuracy: 0.8834\n",
      "Epoch:  52, Training Loss: 0.4045, Training Accuracy: 0.9041, Validation Loss: 0.3554, Validation Accuracy: 0.8845\n",
      "Epoch:  53, Training Loss: 0.3941, Training Accuracy: 0.9050, Validation Loss: 0.3447, Validation Accuracy: 0.8865\n",
      "Epoch:  54, Training Loss: 0.3998, Training Accuracy: 0.9055, Validation Loss: 0.3430, Validation Accuracy: 0.8881\n",
      "Epoch:  55, Training Loss: 0.4085, Training Accuracy: 0.9053, Validation Loss: 0.3479, Validation Accuracy: 0.8880\n",
      "Epoch:  56, Training Loss: 0.3940, Training Accuracy: 0.9009, Validation Loss: 0.3545, Validation Accuracy: 0.8824\n",
      "Epoch:  57, Training Loss: 0.3917, Training Accuracy: 0.9050, Validation Loss: 0.3527, Validation Accuracy: 0.8875\n",
      "Epoch:  58, Training Loss: 0.3978, Training Accuracy: 0.9053, Validation Loss: 0.3533, Validation Accuracy: 0.8851\n",
      "Epoch:  59, Training Loss: 0.4062, Training Accuracy: 0.9034, Validation Loss: 0.3596, Validation Accuracy: 0.8836\n",
      "Epoch:  60, Training Loss: 0.3980, Training Accuracy: 0.9056, Validation Loss: 0.3469, Validation Accuracy: 0.8886\n",
      "Epoch:  61, Training Loss: 0.3977, Training Accuracy: 0.9047, Validation Loss: 0.3560, Validation Accuracy: 0.8839\n",
      "Epoch:  62, Training Loss: 0.3984, Training Accuracy: 0.9065, Validation Loss: 0.3423, Validation Accuracy: 0.8882\n",
      "Epoch:  63, Training Loss: 0.3868, Training Accuracy: 0.9049, Validation Loss: 0.3488, Validation Accuracy: 0.8866\n",
      "Epoch:  64, Training Loss: 0.4039, Training Accuracy: 0.9026, Validation Loss: 0.3512, Validation Accuracy: 0.8874\n",
      "Epoch:  65, Training Loss: 0.3926, Training Accuracy: 0.9059, Validation Loss: 0.3492, Validation Accuracy: 0.8872\n",
      "Epoch:  66, Training Loss: 0.3938, Training Accuracy: 0.9065, Validation Loss: 0.3431, Validation Accuracy: 0.8872\n",
      "Epoch:  67, Training Loss: 0.3863, Training Accuracy: 0.9066, Validation Loss: 0.3430, Validation Accuracy: 0.8878\n",
      "Epoch:  68, Training Loss: 0.3840, Training Accuracy: 0.9053, Validation Loss: 0.3480, Validation Accuracy: 0.8860\n",
      "Epoch:  69, Training Loss: 0.3835, Training Accuracy: 0.9071, Validation Loss: 0.3444, Validation Accuracy: 0.8896\n",
      "Epoch:  70, Training Loss: 0.4022, Training Accuracy: 0.9080, Validation Loss: 0.3464, Validation Accuracy: 0.8888\n",
      "Epoch:  71, Training Loss: 0.4005, Training Accuracy: 0.9061, Validation Loss: 0.3467, Validation Accuracy: 0.8872\n",
      "Epoch:  72, Training Loss: 0.3895, Training Accuracy: 0.9043, Validation Loss: 0.3495, Validation Accuracy: 0.8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  73, Training Loss: 0.3867, Training Accuracy: 0.9057, Validation Loss: 0.3463, Validation Accuracy: 0.8862\n",
      "Epoch:  74, Training Loss: 0.3820, Training Accuracy: 0.9072, Validation Loss: 0.3448, Validation Accuracy: 0.8872\n",
      "Epoch:  75, Training Loss: 0.3848, Training Accuracy: 0.9076, Validation Loss: 0.3407, Validation Accuracy: 0.8896\n",
      "Epoch:  76, Training Loss: 0.3924, Training Accuracy: 0.9089, Validation Loss: 0.3429, Validation Accuracy: 0.8891\n",
      "Epoch:  77, Training Loss: 0.3817, Training Accuracy: 0.9080, Validation Loss: 0.3431, Validation Accuracy: 0.8880\n",
      "Epoch:  78, Training Loss: 0.3768, Training Accuracy: 0.9083, Validation Loss: 0.3450, Validation Accuracy: 0.8859\n",
      "Epoch:  79, Training Loss: 0.3756, Training Accuracy: 0.9077, Validation Loss: 0.3436, Validation Accuracy: 0.8882\n",
      "Epoch:  80, Training Loss: 0.3855, Training Accuracy: 0.9087, Validation Loss: 0.3426, Validation Accuracy: 0.8898\n",
      "Epoch:  81, Training Loss: 0.3870, Training Accuracy: 0.9083, Validation Loss: 0.3450, Validation Accuracy: 0.8866\n",
      "Epoch:  82, Training Loss: 0.3852, Training Accuracy: 0.9069, Validation Loss: 0.3438, Validation Accuracy: 0.8868\n",
      "Epoch:  83, Training Loss: 0.3931, Training Accuracy: 0.9092, Validation Loss: 0.3408, Validation Accuracy: 0.8895\n",
      "Epoch:  84, Training Loss: 0.3798, Training Accuracy: 0.9075, Validation Loss: 0.3443, Validation Accuracy: 0.8880\n",
      "Epoch:  85, Training Loss: 0.3776, Training Accuracy: 0.9083, Validation Loss: 0.3402, Validation Accuracy: 0.8888\n",
      "Epoch:  86, Training Loss: 0.3797, Training Accuracy: 0.9072, Validation Loss: 0.3436, Validation Accuracy: 0.8874\n",
      "Epoch:  87, Training Loss: 0.3785, Training Accuracy: 0.9092, Validation Loss: 0.3418, Validation Accuracy: 0.8888\n",
      "Epoch:  88, Training Loss: 0.3870, Training Accuracy: 0.9086, Validation Loss: 0.3371, Validation Accuracy: 0.8904\n",
      "Epoch:  89, Training Loss: 0.3771, Training Accuracy: 0.9104, Validation Loss: 0.3350, Validation Accuracy: 0.8922\n",
      "Epoch:  90, Training Loss: 0.3758, Training Accuracy: 0.9092, Validation Loss: 0.3417, Validation Accuracy: 0.8899\n",
      "Epoch:  91, Training Loss: 0.3905, Training Accuracy: 0.9094, Validation Loss: 0.3417, Validation Accuracy: 0.8911\n",
      "Epoch:  92, Training Loss: 0.3817, Training Accuracy: 0.9108, Validation Loss: 0.3432, Validation Accuracy: 0.8904\n",
      "Epoch:  93, Training Loss: 0.3732, Training Accuracy: 0.9100, Validation Loss: 0.3386, Validation Accuracy: 0.8908\n",
      "Epoch:  94, Training Loss: 0.3804, Training Accuracy: 0.9101, Validation Loss: 0.3396, Validation Accuracy: 0.8908\n",
      "Epoch:  95, Training Loss: 0.3847, Training Accuracy: 0.9123, Validation Loss: 0.3312, Validation Accuracy: 0.8921\n",
      "Epoch:  96, Training Loss: 0.3849, Training Accuracy: 0.9109, Validation Loss: 0.3365, Validation Accuracy: 0.8922\n",
      "Epoch:  97, Training Loss: 0.3820, Training Accuracy: 0.9098, Validation Loss: 0.3458, Validation Accuracy: 0.8891\n",
      "Epoch:  98, Training Loss: 0.3874, Training Accuracy: 0.9116, Validation Loss: 0.3378, Validation Accuracy: 0.8909\n",
      "Epoch:  99, Training Loss: 0.3780, Training Accuracy: 0.9106, Validation Loss: 0.3407, Validation Accuracy: 0.8899\n",
      "Epoch: 100, Training Loss: 0.3777, Training Accuracy: 0.9108, Validation Loss: 0.3424, Validation Accuracy: 0.8919\n",
      "Finished Training!\n",
      "The training process costs: 113.26062607765198 seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saving test prediction into the directory of ../Output/\n"
     ]
    }
   ],
   "source": [
    "# record starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize Multilayer Perceptron\n",
    "nn = MLP([128, 64, 32, 16, 10], [None, 'relu', 'relu', 'relu', 'softmax'])\n",
    "\n",
    "# train model\n",
    "performance = nn.fit(\n",
    "    X_train_whitening,\n",
    "    y_train,\n",
    "    X_validation_whitening,\n",
    "    y_validation,\n",
    "    learning_rate=4e-4,\n",
    "    epochs=100,\n",
    "    weight_decay=1e-1,\n",
    "    momentum=0.1,\n",
    "    dropout=0.1,\n",
    "    batch_size=256\n",
    "    )\n",
    "\n",
    "# record ending time and print time cost of training process\n",
    "end_time = time.time()\n",
    "print('The training process costs: ' + str(end_time - start_time) + ' seconds.')\n",
    "plot_result(*performance)\n",
    "\n",
    "# perform prediction on test data set and save the result into ../Output directory\n",
    "test_prediction = nn.predict(X_test_whitening)\n",
    "with h5py.File('../Output/Predicted_labels.h5','w') as H:\n",
    "        H.create_dataset('label', data=test_prediction)\n",
    "print('Successfully saving test prediction into the directory of ../Output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
